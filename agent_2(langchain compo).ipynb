{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **PromptTemplate**: Allows reusable templates.\n",
    "2. **Nodes**: Agents or functions\n",
    "3. **Edges**: connect nodes\n",
    "4. **Connectional Edges**: decisions\n",
    "5. **Agent State**: \n",
    "    - Agent State is accessible to all parts of the graph\n",
    "    - It is local to the graph\n",
    "    - Can be stored in a persistence layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AgentState(TypedDict):\n",
    "#         messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AgentState(TypedDict):\n",
    "#     input: str\n",
    "#     chat_history: list[BaseMeassage]\n",
    "#     agent_outcome: Union[AgentAction, AgentFinish, Name]\n",
    "#     intermediate_step: Annotated[list[tuple[AgentAction, str]], operated.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code High Level Overview**\n",
    "\n",
    "1. llm:call_ll\n",
    "2. condn_edge:exists_action\n",
    "3. action:take_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# to construct agent state\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# to construct human, ai and system messages\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# llm \n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TAVILY_API_KEY = \"tvly-IsSIq4iFinMqzSVRyEa...\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.nbcnews.com/news/us-news/live-blog/live-updates-burning-man-flooding-keeps-thousands-stranded-nevada-site-rcna103193',\n",
       "  'content': \"Profile\\nSections\\ntv\\nFeatured\\nMore From NBC\\nFollow NBC News\\nnews Alerts\\nThere are no new alerts at this time\\nBurning Man flooding keeps thousands stranded at Nevada site as authorities investigate 1 death\\nBurning Man attendees struggling to get home\\n70,000+ stuck at Burning Man: When will they be able to get out?\\n Thousands still stranded at Burning Man after torrential rain\\nBurning Man revelers unfazed by deluge and deep mud\\nReuters\\nThousands of Burning Man attendees partied hard on Sunday despite downpours that turned the Nevada desert where the annual arts and music festival takes place into a sea of sticky mud and led officials to order the multitudes to shelter in place.\\n Neal Katyal warns hiking in the mud\\ncan be 'worse than walking on ice'\\nDoha Madani\\nNeal Katyal, the former acting U.S. solicitor general, is among the Burning Man attendees who decided to take the risk and hike out of the festival grounds.\\n Videos posted to his Instagram story show Diplo walking through mud before, he says, he hitchhiked to Gerlach and Reno to make a flight to Washington, D.C.\\n“I just got done DJ’ing for three hours, after walking f---ing for four hours out of the desert and taking a flight, mud still on my face,” he said in a video posted to his Instagram story last night.\\n Burning Man memes are swamping social media\\nAngela Yang\\nAs heavy rain turns Burning Man 2023 into a muddy mess, a deluge of unsympathetic jokes has swamped the internet outside Black Rock City, the temporary location built annually for the nine-day festival in the remote desert of Nevada.\\n\"},\n",
       " {'url': 'https://www.cnn.com/2023/09/05/us/burning-man-storms-shelter-exodus-tuesday/index.html',\n",
       "  'content': \"CNN values your feedback\\nBurning Man attendees make a mass exodus after a dramatic weekend that left thousands stuck in the Nevada desert\\nThousands of Burning Man attendees finally made their mass exodus after intense rain over the weekend flooded camp sites and filled them with thick, ankle-deep mud – stranding more than 70,000 free-spirited revelers as they waited for the Nevada desert city to dry out.\\n Burning Man organizers lift driving ban after heavy rains left the event smothered in mud and trapped thousands\\nThe area was still muddy and parts were still difficult to navigate, organizers warned, and the wait time to leave the city Monday night was about seven hours. Diplo hitchhiked ride out of rain-drenched Burning Man after walking miles 'through the mud' and actually made it to his DC concert\\n“Quite a wet start to September for much of eastern CA-western NV,” the National Weather Service in Reno wrote on X. ” “As soon as the tents started getting water-logged or unlivable, people in RVs started taking in some of the tenters, so everybody was warm,” Kaz Qamruddin, who attended the event, told CNN’s Brianna Keilar Monday.\\n From wood blocks to 'poop buckets,' how Burning Man organizers told festivalgoers to prepare for heavy rain\\nAmong the early departures was music DJ Diplo, who told CNN he walked several miles in the muddy desert Saturday morning along with other celebrities, including Chris Rock, Cindy Crawford, Kaia Gerber and Austin Butler.\"}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool = TavilySearchResults(max_results=2)\n",
    "tool.invoke({\"query\": \"What happened in the latest burning man floods\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.tools.tavily_search.tool.TavilySearchResults'>\n",
      "tavily_search_results_json\n"
     ]
    }
   ],
   "source": [
    "print(type(tool))\n",
    "print(tool.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        # Now let us initialize Graph\n",
    "        # 1st is state graph with Agent State [it does not have any nodes or edges attached to it.]\n",
    "        graph = StateGraph(AgentState)\n",
    "        # Sketch out nodes\n",
    "        graph.add_node(\"llm\",...)\n",
    "        graph.add_node(\"action\",...)\n",
    "        # Now conditional edge. this goes after llm is called, \n",
    "        # it checks whether there is action present. \n",
    "        # If there is, it goes to action node.\n",
    "        #  And if not it goes to the end node\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", # node where the edge starts\n",
    "            ..., # function that will determine where to go after\n",
    "            {True: \"action\", False:END} # dict: representing how to map the response of the fn to the next node to go to. So if the fn returns true, we'll go to action node, else end node.\n",
    "        )\n",
    "        graph.add_node(\"action\", \"llm\") # regular node\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "        def call_openai(self, state: AgentState):\n",
    "            messages = state['messages']\n",
    "            if self.system:\n",
    "                messages = [SystemMessage(content=self.system)] + messages\n",
    "            message = self.model.invoke(messages)\n",
    "            return {'messages': [message]}\n",
    "        \n",
    "        def take_action(self, state: AgentState):\n",
    "            tool_calls = state['messages'][-1].tool_calls\n",
    "            results = []\n",
    "            for t in tool_calls:\n",
    "                print(f\"Calling: {t}\")\n",
    "                if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                    print(\"\\n ....bad tool name....\")\n",
    "                    result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "                else:\n",
    "                    result = self.tools[t['name']].invoke(t['args'])\n",
    "                results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "            print(\"Back to the model!\")\n",
    "            return {'messages': results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redefined the above Agent with Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)##### added\n",
    "        graph.add_node(\"action\", self.take_action)###### added\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,######### added\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1\")\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# Image(abot.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'sf weather'}, 'id': 'ead36170-4423-4b6f-9dcf-950aa5af22f8', 'type': 'tool_call'}\n",
      "Back to the model!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The weather in San Francisco, California currently is sunny with a temperature of 58.5°F (14.7°C) and humidity of 82%. There are no precipitation or clouds expected.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
